{"cells":[{"cell_type":"markdown","metadata":{"id":"ColxfM_LfK4K"},"source":["# KOBERT를 이용해 ESG 분류하기\n"]},{"cell_type":"markdown","source":["`ESG,긍부정 학습 순서`\n","### 1차 학습(model + 추가 layer : requires_grad = True)\n","- 1. model 선언하기\n","- 2. ESG 데이터로더로 1차 학습\n","- 3. model + 추가 layer 전체 state_dict 저장하기\n","\n","### 2차 학습(model + esg 추가 layer + 긍부정 layer)\n","\n","- a. model + esg layer : rg = False, 긍부정 layer : rg = True\n","- b. model + esg layer + 긍부정 layer  : re = True\n","\n","- 1. model 선언하기(1차 학습 state_dict 적용)\n","- 2. 긍부정 데이터로더로 2차 학습\n","- 3. model,esg,긍부정 layer 모두 state_dict 저장하기\n"],"metadata":{"id":"dy60SeI8_cm7"}},{"cell_type":"markdown","metadata":{"id":"WAxoxUV-fK4M"},"source":["`참고 블로그 모음 `\n","\n","# 모델링 관련 참고 사이트\n","\n","### kobert modeling code\n","- kobert 감정분류 코드 : https://velog.io/@sseq007/Kobert-%EB%AA%A8%EB%8D%B8-%EC%82%AC%EC%9A%A91\n","\n","### 파라미터 저장 및 재적용 관련 참고 사이트\n","- pytorch 공식 홈페이지 모델 저장하고 불러오기 : https://tutorials.pytorch.kr/beginner/saving_loading_models.html\n","\n","### requires_grad = True 일부 계층 적용 관련 참고 사이트\n","- 개인 블로그 : https://jeonghyeokpark.netlify.app/pytorch/2020/11/27/pytorch1.html\n","\n","### 모델의 파라미터 접근하기\n","- 개인 블로그 : https://soundprovider.tistory.com/entry/pytorch-torch%EC%97%90%EC%84%9C-parameter-%EC%A0%91%EA%B7%BC%ED%95%98%EA%B8%B0\n","- 개인 블로그 : https://dbwp031.tistory.com/25\n","\n","### 파이토치 기초 정리\n","- 이수안컴퓨터연구소 파이토치 기초 정리 : https://www.youtube.com/watch?v=k60oT_8lyFw&t=8494s\n","\n","### 모델의 학습모드와 평가모드 설명\n","- 개인 블로그 : https://tigris-data-science.tistory.com/entry/PyTorch-modeltrain-vs-modeleval-vs-torchnograd\n","\n","### 데이터 프레임 loc를 이용한 값 변경\n","- 개인 블로그 : https://velog.io/@skkumin/Pandas-loc%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84-%EA%B0%92-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0\n","\n","### train_test_split 사용방법\n","- 개인 블로그 : https://smalldatalab.tistory.com/23\n","\n","### Dataset과 DataLoader, transform 사용방법\n","- Dataset & DataLoader 파이토치 튜토리얼 : https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html\n","- transform : 파이토치 튜토리얼 : https://tutorials.pytorch.kr/beginner/basics/transforms_tutorial.html\n","- 개인 블로그 : https://curiousseed.tistory.com/76\n","\n","### gluonnlp의 bertsentenceTransform 설명\n","- gluonnlp 공식 사이트 : https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.BERTSentenceTransform\n","\n","### model.zero_grad vs optimizer.zero_grad 차이점 설명\n","- 개인 블로그 : https://otzslayer.github.io/pytorch/2022/04/17/difference-between-zero-grads.html\n","\n","### 구글 코랩에서 모델 저장하는 방법\n","- 개인 블로그 : https://velog.io/@bpbpbp_yosep/PyTorch%EC%97%90%EC%84%9C-%EB%AA%A8%EB%8D%B8-%EC%A0%80%EC%9E%A5%ED%95%98%EA%B8%B0\n","\n","### 드라이브 파일 구글 코랩에 업로드 하는 방법\n","- 개인 블로그 : https://rfriend.tistory.com/m/564"]},{"cell_type":"markdown","metadata":{"id":"xmjhbDCEumPC"},"source":["`필요 환경 및 패키지 설치`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSkf0n9pfK4N"},"outputs":[],"source":["!pip install mxnet\n","!pip install pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch\n","!pip install gluonnlp==0.10.0\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1KvuoVJkrgM"},"outputs":[],"source":["# 오류 메세지 frames 열고 onp.bool -> bool로 변경하기\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","import urllib.request\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","kobert_tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1') # input을 만드는 tokenize 명령을 할 수 있는 tokenizer\n","\n","kobertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False) # 코버트 모델 불러오기\n","\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(kobert_tokenizer.vocab_file, padding_token='[PAD]') # tokenize의 기반이 되는 사전"]},{"cell_type":"markdown","metadata":{"id":"UqpH3JnkBJqq"},"source":["`kobert_tokenizer 이해 예시`"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gAxDWDwNBJqr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713229861738,"user_tz":-540,"elapsed":382,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"2d5940f2-c426-4018-9ae6-ccc629419761"},"outputs":[{"output_type":"stream","name":"stdout","text":["kobert_tokenizer\n","{'input_ids': [2, 1375, 4384, 6896, 517, 5330, 5439, 3072, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","kobert_tokenizer.tokenize\n","['▁나는', '▁집', '에', '▁', '가', '고', '▁싶다']\n"]}],"source":["word_tokenizer = kobert_tokenizer.tokenize\n","\n","# kobert_tokenizer(문장) vs kobert_tokenizer.tokenize(문장) 비교하기\n","\n","print('kobert_tokenizer')\n","\n","print(kobert_tokenizer('나는 집에 가고 싶다')) # 단어를 분리하고 임베딩까지 한다.\n","\n","print('kobert_tokenizer.tokenize')\n","\n","print(word_tokenizer('나는 집에 가고 싶다')) # 단어를 분리한다.\n","\n","# kobert_tokenizer(문장)이 반환하는 항목 설명\n","\n","# {\n","#     input_ids : [스페셜 토큰이 포함된 문장 임베딩]\n","#     token_type_ids : [문장의 구분을 나누는 문장 임베딩 첫 문장 0]\n","#     attention_mask : [문장 길이가 다를 때 긴 것에 기준을 맞추고 짧은 것은 패딩 | 패딩 된 부분 : 0 (0이어서 계산에 반영되는 않는 무시의 용도), 패딩 안 된 부분 : 1]\n","# }"]},{"cell_type":"markdown","source":["` 구글 드라이브 연동하기`"],"metadata":{"id":"yFDNnwrmJCkM"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 현재 경로 확인하는 법\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kxxxbKPk0qZ","executionInfo":{"status":"ok","timestamp":1713229883982,"user_tz":-540,"elapsed":18548,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"6a9a9185-4a5d-4385-aef9-23bf022f96b6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content\n"]}]},{"cell_type":"markdown","source":["`사전 파라미터 지정`"],"metadata":{"id":"VuMpkitVIgXl"}},{"cell_type":"code","source":["max_len = 128\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"metadata":{"id":"2ttg1EPIIfem","executionInfo":{"status":"ok","timestamp":1713229894852,"user_tz":-540,"elapsed":361,"user":{"displayName":"한상혁","userId":"05319193929852015189"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["`esg 데이터셋 불러오기 및 dataloader 구축`"],"metadata":{"id":"0AcJ-SoEHU9X"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"02gztnFdu6p6","executionInfo":{"status":"ok","timestamp":1713229970071,"user_tz":-540,"elapsed":12983,"user":{"displayName":"한상혁","userId":"05319193929852015189"}}},"outputs":[],"source":["# esg 데이터 불러오기\n","esg_data = pd.read_csv('/content/drive/MyDrive/kobert_modeling/문장esg테스트용.csv')\n","\n","# 필요없는 정보 제거하기\n","del esg_data['Unnamed: 0']\n","\n","# 문장 라벨 묶어주기\n","esg_data_list = []\n","for sent, e_label, s_label, g_label in zip(esg_data['문장'], esg_data['E'], esg_data['S'], esg_data['G']):\n","    data = []\n","    data.append(str(sent))\n","    data.append((str(e_label), str(s_label), str(g_label)))\n","\n","    esg_data_list.append(data)\n","\n","# train_test_split 활용해서 train_data와 valid_data 구분하기\n","from sklearn.model_selection import train_test_split\n","esg_train_data, esg_valid_data = train_test_split(esg_data_list, test_size = 0.2, shuffle = True, random_state = 32)\n","\n","# kobert 모델에 input으로 들어갈 데이터셋(BERTDataset)을 만들어주는 클래스\n","class ESG_BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","\n","        # Kobert 모델에 들어가기 위한 input을 만드는 함수입니다.\n","\n","        # dataset: 내 데이터셋 : [문장, 라벨]\n","        # sent_idx: 0 : 문장 index\n","        # label_idx: 1 : 라벨 index\n","        # bert_tokenizer: 사용할 tokenizer : 사전에 정의하고 생성해야 함.\n","        # max_len: input sentence의 최대 길이\n","        # pad: (True/False) : max_len을 고려한 패딩 여부 결정\n","        # pair: (True/False) : input이  [sent a, sent b]처럼 문장쌍으로 들어가는지 여부\n","\n","        self.transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab = vocab, pad=pad, pair=pair)\n","\n","        # nlp.data.BERTSentenceTransform() 사용 방법\n","\n","        # input 정리\n","        # tokenizer(bert_tokenizer)\n","        # max_seq_length : 위 참고\n","        # vocab : CLS, SEP 등등의 토큰이 임베딩 되어 있는 사전\n","        # pad : 위 참고\n","        # pair : 위 참고\n","\n","        # output 정리\n","        # token_ids : vocab에 따른 임베딩\n","        # valid_len : 문장의 실제 길이\n","        # tokekn_type : 문장 pair 여부를 보여주는 0,1로 구성된 positional encoding\n","\n","\n","        self.sentences = [self.transform([i[sent_idx]]) for i in dataset] # 데이터 셋의 문장들을 bert 맞춤형 토큰화 모음 리스트\n","        self.labels = torch.tensor([(int(i[label_idx][0]), int(i[label_idx][1]), int(i[label_idx][2])) for i in dataset]) # 데이터 셋의 라벨들 모음 리스트\n","        # self.labels 형태 예시 : (0,1,0)\n","\n","\n","    def __getitem__(self, i): # 특정 문장 특정 문장의 라벨을 리턴해줌\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels)) # 문장 개수 리턴 해줌.\n","\n","\n","esg_data_train = ESG_BERTDataset(esg_train_data, 0, 1, word_tokenizer, vocab, max_len, True, False)\n","esg_data_test = ESG_BERTDataset(esg_valid_data, 0, 1, word_tokenizer, vocab, max_len, True, False)\n","\n","\n","# DataLoader의 역할 : 한번에 batch_size만큼 학습시키는(테스트 하는) iterable 생성\n","esg_train_dataloader = torch.utils.data.DataLoader(esg_data_train, batch_size = batch_size, num_workers = 5)\n","esg_test_dataloader = torch.utils.data.DataLoader(esg_data_test, batch_size = batch_size, num_workers = 5)"]},{"cell_type":"markdown","source":["`긍부정 데이터셋 불러오기 및 dataloader 구축`"],"metadata":{"id":"rZgT_mNlEC5u"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n","\n","# 긍정 1 부정 0\n","pn_train_data = pd.read_table('ratings_train.txt')\n","pn_test_data = pd.read_table('ratings_test.txt')\n","# pn_train_data\n","pn_train_data_list = []\n","for sent, label in zip(pn_train_data['document'], pn_train_data['label']):\n","    pn_train_data = []\n","    pn_train_data.append(str(sent))\n","    pn_train_data.append(str(label))\n","\n","    pn_train_data_list.append(pn_train_data)\n","\n","#pn_test_data\n","pn_test_data_list = []\n","for sent, label in zip(pn_test_data['document'], pn_test_data['label']):\n","    pn_test_data = []\n","    pn_test_data.append(str(sent))\n","    pn_test_data.append(str(label))\n","\n","    pn_test_data_list.append(pn_test_data)\n","\n","class SENT_BERTDataset(Dataset): # bert 모델에 어떤 데이터셋을 넣을 줄 것인지 결정 : 어떤 데이터셋을 받아와서 어떻게 token화 해서 넣을 거야??? 를 결정한다.\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        self.transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab = vocab, pad=pad, pair=pair)\n","        self.sentences = [self.transform([i[sent_idx]]) for i in dataset] # 데이터 셋의 문장들을 bert 맞춤형 토큰화 모음 리스트\n","        self.labels = [np.int32(i[label_idx]) for i in dataset] # 데이터 셋의 라벨들 모음 리스트\n","\n","    def __getitem__(self, i): # 특정 문장 특정 문장의 라벨을 리턴해줌\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels)) # 문장 개수 리턴 해줌.\n","\n","\n","pn_data_train = SENT_BERTDataset(pn_train_data_list, 0, 1, word_tokenizer, vocab, max_len, True, False)\n","pn_data_test = SENT_BERTDataset(pn_test_data_list, 0, 1, word_tokenizer, vocab, max_len, True, False)\n","\n","# DataLoader의 역할 : 한번에 batch_size만큼 시키는 iterable 생성\n","pn_train_dataloader = torch.utils.data.DataLoader(pn_data_train, batch_size = batch_size, num_workers = 5)\n","pn_test_dataloader = torch.utils.data.DataLoader(pn_data_test, batch_size = batch_size, num_workers = 5)"],"metadata":{"id":"zJXfEAuIEJaX","executionInfo":{"status":"ok","timestamp":1713229999128,"user_tz":-540,"elapsed":23677,"user":{"displayName":"한상혁","userId":"05319193929852015189"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["`--------------------------------------모델 구축----------------------------------------`\n","- 과정1. esg 데이터셋을 esg분류 모델에 넣는다. 이때 esg 추가 계층의 requires_grad만 True로 한다.\n","- 과정2. 학습을 시킨 후 esg 추가 계층의 state_dict만 저장한다.\n","- 과정3. 긍부정 데이터셋을 긍부정분류 모델에 넣는다. 이때 esg 추가 계층의 requires_grad는 False 이고 긍부정분류 계층의 requires_grad = True이다.\n","- 과정4. 학습을 시킨 후 긍부정 추가 계층의 state_dict만 저장한다\n","- 과정5. 두가지 추가 계층을 모두 가진 모델을 구축하고 2,4에서 저장한 state_dict를 각각의 계층에 적용시킨다."],"metadata":{"id":"c_V6qVeIqqc2"}},{"cell_type":"markdown","metadata":{"id":"_1j-Bg4Kmq6J"},"source":["# ESG를 분류하는 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"085FcR0HcBYx"},"outputs":[],"source":["class BERT_ESG_Classifier(nn.Module):\n","    def __init__(self,\n","                 bert, # bert 모델 받아오기\n","                 hidden_size = 768, # 은닉층의 크기(기준 수)\n","                 num_classes = 3,   # [E,S,G]\n","                 dr_rate = None, # dropout_rate : 신경망 학습 중에 일부 뉴런을 무작위로 제거하여 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법\n","                 params = None):\n","        super(BERT_ESG_Classifier, self).__init__()\n","        self.bert = bert # 사용할 bert 모델 지정\n","        self.dr_rate = dr_rate # dropout 비율 지정\n","        self.esg_classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate: # dr_rate가 0과 None이 아니고 (0~1) 사이로 설정되었을 경우에는\n","            self.dropout = nn.Dropout(p = dr_rate) # nn.dropout을 실행한다.\n","        # (E,S,G)와의 연관성을 0~1 사이로 보여주는 sigmoid 함수 추가\n","        self.activation_sigmoid = nn.Sigmoid()\n","\n","    def gen_attention_mask(self, token_ids, valid_length): # attention mask를 생성하는 것 : 이 문장을 바라보는 전문가들 생성한다.\n","        attention_mask = torch.zeros_like(token_ids) # token_ids와 같은 크기를 가지고 있는 0으로 채워지는 것들  : 뭔가 포지셔널 인코딩일 것 같은 느낌\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1 # 가리지 않는 부분 설정하기\n","        return attention_mask.float() # dtype 은 float32로 한다.\n","\n","    def forward(self, token_ids, valid_length, segment_ids): # segment_ids 는 문장단위를 나누는 임베딩 부분인 것 같다. 한 문장일 경우 0000, 두 문장 이상일 경우 00..111\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        # torch.long() : int64 타입\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict = False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        esg_score = self.esg_classifier(out)\n","        activation_esg_score = self.activation_sigmoid(esg_score)\n","        return activation_esg_score"]},{"cell_type":"markdown","source":["# ESG 모델 + 긍부정 분류 모델"],"metadata":{"id":"0mpZUIwCeDqQ"}},{"cell_type":"code","source":["class BERT_SENT_Classifier(nn.Module):\n","    def __init__(self,\n","                 bert, # bert 모델 받아오기\n","                 hidden_size = 768, # 은닉층의 크기(기준 수)\n","                 num_classes = 3,   # [E,S,G]\n","                 dr_rate = None, # dropout_rate : 신경망 학습 중에 일부 뉴런을 무작위로 제거하여 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법\n","                 params = None):\n","        super(BERT_ESG_Classifier, self).__init__()\n","        self.bert = bert # 사용할 bert 모델 지정\n","        self.dr_rate = dr_rate # dropout 비율 지정\n","        self.esg_classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate: # dr_rate가 0과 None이 아니고 (0~1) 사이로 설정되었을 경우에는\n","            self.dropout = nn.Dropout(p = dr_rate) # nn.dropout을 실행한다.\n","        # (E,S,G)와의 연관성을 0~1 사이로 보여주는 sigmoid 함수 추가\n","        self.activation_sigmoid = nn.Sigmoid()\n","        self.sent_clssifier = nn.Linear(num_classes, 2)\n","        self.activation_softmax = nn.Softmax(dim = 1)\n","\n","    def gen_attention_mask(self, token_ids, valid_length): # attention mask를 생성하는 것 : 이 문장을 바라보는 전문가들 생성한다.\n","        attention_mask = torch.zeros_like(token_ids) # token_ids와 같은 크기를 가지고 있는 0으로 채워지는 것들  : 뭔가 포지셔널 인코딩일 것 같은 느낌\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1 # 가리지 않는 부분 설정하기\n","        return attention_mask.float() # dtype 은 float32로 한다.\n","\n","    def forward(self, token_ids, valid_length, segment_ids): # segment_ids 는 문장단위를 나누는 임베딩 부분인 것 같다. 한 문장일 경우 0000, 두 문장 이상일 경우 00..111\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        # torch.long() : int64 타입\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict = False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        esg_score = self.esg_classifier(out)\n","        posneg = self.sent_classifier(esg_score)\n","        activation_sent_score = self.activation_softmax(posneg)\n","        return activation_sent_score"],"metadata":{"id":"A6Xe0yNUeC1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 최종 esg/긍부정 모델"],"metadata":{"id":"kC3VjlIEp_Q3"}},{"cell_type":"code","source":["class BERT_ESG_SENT_Classifier(nn.Module):\n","    def __init__(self,\n","                 bert, # bert 모델 받아오기\n","                 hidden_size = 768, # 은닉층의 크기(기준 수)\n","                 num_classes = 3,   # [E,S,G]\n","                 dr_rate = None, # dropout_rate : 신경망 학습 중에 일부 뉴런을 무작위로 제거하여 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법\n","                 params = None):\n","        super(BERT_ESG_Classifier, self).__init__()\n","        self.bert = bert # 사용할 bert 모델 지정\n","        self.dr_rate = dr_rate # dropout 비율 지정\n","        self.esg_classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate: # dr_rate가 0과 None이 아니고 (0~1) 사이로 설정되었을 경우에는\n","            self.dropout = nn.Dropout(p = dr_rate) # nn.dropout을 실행한다.\n","        # (E,S,G)와의 연관성을 0~1 사이로 보여주는 sigmoid 함수 추가\n","        self.activation_sigmoid = nn.Sigmoid()\n","        self.sent_clssifier = nn.Linear(num_classes, 2)\n","        self.activation_softmax = nn.Softmax(dim = 1)\n","\n","    def gen_attention_mask(self, token_ids, valid_length): # attention mask를 생성하는 것 : 이 문장을 바라보는 전문가들 생성한다.\n","        attention_mask = torch.zeros_like(token_ids) # token_ids와 같은 크기를 가지고 있는 0으로 채워지는 것들  : 뭔가 포지셔널 인코딩일 것 같은 느낌\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1 # 가리지 않는 부분 설정하기\n","        return attention_mask.float() # dtype 은 float32로 한다.\n","\n","    def forward(self, token_ids, valid_length, segment_ids): # segment_ids 는 문장단위를 나누는 임베딩 부분인 것 같다. 한 문장일 경우 0000, 두 문장 이상일 경우 00..111\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        # torch.long() : int64 타입\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict = False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        esg_score = self.esg_classifier(out)\n","        activation_esg_score = self.activation_sigmoid(esg_score)\n","\n","        posneg = self.sent_classifier(esg_score)\n","        activation_sent_score = self.activation_softmax(posneg)\n","        return activation_esg_score, activation_sent_score"],"metadata":{"id":"vI_lb-6ppyQ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTa0AGa3BJqv"},"source":["`모델 정의하기`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HB9xXUOow9ej"},"outputs":[],"source":["# BERT  모델 불러오기\n","model = BERT_ESG_Classifier(kobertmodel,  dr_rate = 0.5).to(device)"]},{"cell_type":"code","source":["# freezing 결과 확인하기\n","for name, para in model.named_parameters() :\n","    print(para.requires_grad)\n"],"metadata":{"id":"U3VckVXK8B9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"POmNPbYEBJqv"},"source":["`모델에 호환되는 optimizer, loss_function, scheduler 정의`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqjivlh51q4K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713187784171,"user_tz":-540,"elapsed":448,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"d2c01354-ff68-4b70-ad56-bae887a3989e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# 옵티마이저 생성 시 전달해줄 파라미터 정의\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [ # model.named_parameters()는 model의 해당층이름/ 해당층 가중치를 리턴\n","    {'params': [weight for name, weight in model.named_parameters() if not any(nd in name for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [weight for name, weight in model.named_parameters() if any(nd in name for nd in no_decay)], 'weight_decay': 0.0} ]\n","\n","\n","# 옵티마이저 정의 : model로 부터 온 것 : model과 연관\n","optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n","\n","# 손실함수 정의\n","loss_fn = nn.BCELoss()\n","\n","# 스케쥴러 생성 시 전달해줄 파라미터 정의\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","# 스케쥴러 정의 : model로 부터 온 것 : model과 연관\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)"]},{"cell_type":"markdown","source":["`optimizer vs scheduler`\n","- optimizer : 가중치 업데이트를 어떤 방식으로 할 것인가?\n","- scheduler : 학습률을 어떤 방식으로 조절할 것인가?"],"metadata":{"id":"YkYLSdARAwyV"}},{"cell_type":"markdown","metadata":{"id":"ksWVE9G-BJqw"},"source":["`정확도 계산하는 함수 정의`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMAGcjy95fHa"},"outputs":[],"source":["# esg 분류 시에 사용한다\n","def esg_calc_accuracy(X,Y):\n","    X[X <= 0.5 ] = 0\n","    X[X > 0.5 ] = 1\n","    answer = 0\n","    for pred in (X - Y) :\n","        if torch.abs(pred).sum() == 0 :\n","            answer += 1\n","    train_acc = answer/batch_size\n","    return train_acc\n","\n","# 긍부정 분류 시에 사용한다.\n","def sent_calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"markdown","metadata":{"id":"d-SfDpRK12-v"},"source":["`모델 학습 코드`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270,"referenced_widgets":["6521480233c5471887a798dc9dc6cc37","4887f5e47e8a4a2587d8af6dca8ac290","a4445e0346fe41019a3b0e17cfd9fe4b","a5d718a83abd4205b3489d4a4d2e5395","eafc6a3ca3a1438381d1a504a5f25603","c212f2dc79584bb1ab21a0fd526aefb9","4cb5f789c1ca40df8e2b576682ed9465","10f7e3f2aed74451a3431fc862d9ef6f","3939000377a144cab85f69040daed1d1","bea45ac188634d8cb4c8b33820978dc8","924373d1abf3449fa41bb0191f0c8318"]},"executionInfo":{"elapsed":1098287,"status":"ok","timestamp":1713189411669,"user":{"displayName":"한상혁","userId":"05319193929852015189"},"user_tz":-540},"id":"8yCn_oqY5_7S","outputId":"879dd76a-ed46-4024-fbaa-5f23c5b7183d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-35-a2bb859b5912>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/951 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6521480233c5471887a798dc9dc6cc37"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["1 번 반복 | 배치순서 1 | 오차 정도 1.612990140914917| 정확도 0.125\n","1 번 반복 | 배치순서 201 | 오차 정도 1.3686542510986328| 정확도 0.39552238805970147\n","1 번 반복 | 배치순서 401 | 오차 정도 1.3103528022766113| 정확도 0.427213216957606\n","1 번 반복 | 배치순서 601 | 오차 정도 1.1976993083953857| 정확도 0.43612208818635606\n","1 번 반복 | 배치순서 801 | 오차 정도 1.3548005819320679| 정확도 0.4432740324594257\n","epoch 1 train acc 0.4473416140904311\n"]}],"source":["# esg 분류 시에 사용한다.\n","for e in range(1):\n","    train_acc = 0.0\n","    model.train() # model을 훈련모드로 바꾸고, 가중치가 업데이트 될 수 있게 한다.\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(esg_train_dataloader)):\n","        # 옵티마이저의 미분값을 0으로 초기화\n","        optimizer.zero_grad()\n","\n","        # model의 forward 인자 설정\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.float().to(device)\n","\n","        # model output 도출\n","        out = model.forward(token_ids, valid_length, segment_ids)\n","\n","        # 모델 output과 label(정답)과의 손실함수 정의\n","        loss = loss_fn(out, label)\n","\n","        # 손실함수의 기울기 계산\n","        loss.backward()\n","\n","        # gradient vanishing 또는 gradient exploding을 방지하기 위한 gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","        # 기울기 반영한 가중치 업데이트\n","        optimizer.step()\n","        scheduler.step()\n","\n","        train_acc += esg_calc_accuracy(out, label)\n","\n","        if batch_id % log_interval == 0:\n","             print(f'{e+1} 번 반복 | 배치순서 {batch_id + 1} | 오차 정도 {loss.data.cpu().numpy()}| 정확도 {train_acc / (batch_id+1)}')\n","\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))"]},{"cell_type":"code","source":["# 감정 분류 시에 사용한다.\n","for e in range(1):\n","    train_acc = 0.0\n","    model.train() # model을 훈련모드로 바꾸고, 가중치가 업데이트 될 수 있게 한다.\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(pn_train_dataloader)):\n","        # 옵티마이저의 미분값을 0으로 초기화\n","        optimizer.zero_grad()\n","\n","        # model의 forward 인자 설정\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        # model output 도출\n","        out = model.forward(token_ids, valid_length, segment_ids)\n","\n","        # 모델 output과 label(정답)과의 손실함수 정의\n","        loss = loss_fn(out, label)\n","\n","        # 손실함수의 기울기 계산\n","        loss.backward()\n","\n","        # gradient vanishing 또는 gradient exploding을 방지하기 위한 gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","        # 기울기 반영한 가중치 업데이트\n","        optimizer.step()\n","        scheduler.step()\n","\n","        train_acc += sent_calc_accuracy(out, label)\n","\n","        if batch_id % log_interval == 0:\n","             print(f'{e+1} 번 반복 | 배치순서 {batch_id + 1} | 오차 정도 {loss.data.cpu().numpy()}| 정확도 {train_acc / (batch_id+1)}')\n","\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))"],"metadata":{"id":"ihR-KZotr3cN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SXsq_T5z2JvX"},"source":["`학습 완료시킨 model을 이용해서 test(predict) 해보기`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiTjceBh1-xn"},"outputs":[],"source":["# esg 분류시에 사용한다.\n","def esg_predict(predict_sentence): # input = 감정분류하고자 하는 sentence\n","\n","    data = [predict_sentence,(0,0,0)]\n","    dataset_another = [data]\n","\n","    another_test = ESG_BERTDataset(dataset_another, 0, 1, word_tokenizer, vocab, max_len, True, False) # 토큰화한 문장\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size = batch_size, num_workers = 5) # torch 형식 변환\n","\n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        # model의 forward 인자 설정\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.float().to(device)\n","        # model output 도출\n","        out = model(token_ids, valid_length, segment_ids)\n","        # 모델의 output은 바로 사용하지 못한다. 밑에 있는 logits 코드 이용하기\n","        for i in out:\n","            logits = i\n","            logits = logits.detach().cpu().numpy()\n","\n","    return logits\n"]},{"cell_type":"code","source":["def sent_predict(predict_sentence): # input = 감정분류하고자 하는 sentence\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","\n","    another_test = SENT_BERTDataset(dataset_another, 0, 1, word_tokenizer, vocab, max_len, True, False) # 토큰화한 문장\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size = batch_size, num_workers = 5) # torch 형식 변환\n","\n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        # model의 forward 인자 설정\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","        # model output 도출\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","\n","        test_eval = []\n","        for i in out: # out = model(token_ids, valid_length, segment_ids)\n","            logits = i\n","            logits = logits.detach().cpu().numpy()\n","\n","            if np.argmax(logits) == 0:\n","                test_eval.append('e')\n","            elif np.argmax(logits) == 1:\n","                test_eval.append(\"s\")\n","            elif np.argmax(logits) == 2:\n","                test_eval.append(\"g\")\n","\n","\n","    return test_eval[0]"],"metadata":{"id":"yc-IGeh-sPT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터 불러오기\n","test_data = pd.read_csv('/content/drive/MyDrive/kobert_modeling/naver_news_test.csv')\n","\n","# 문장 추출\n","sents = test_data['content']\n","for i, sent in enumerate(sents[:10]) :\n","    esg_output = predict(sent)\n","    print(f'{i+1}번 문장에 대한 esg 등급입니다.')\n","    print(f' E : {esg_output[0] * 100}점, S : {esg_output[1] * 100}점, G : {esg_output[2] * 100}점')"],"metadata":{"id":"m8ZQ88il-J1T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713191489008,"user_tz":-540,"elapsed":3249,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"e8a2e2bc-805b-4362-86b8-0dfd0268e947"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["1번 문장에 대한 esg 등급입니다.\n"," E : 51.00491642951965점, S : 59.80818271636963점, G : 42.548397183418274점\n","2번 문장에 대한 esg 등급입니다.\n"," E : 64.41511511802673점, S : 0.1727480790577829점, G : 99.80581402778625점\n","3번 문장에 대한 esg 등급입니다.\n"," E : 64.37705159187317점, S : 0.174202723428607점, G : 99.82020854949951점\n","4번 문장에 대한 esg 등급입니다.\n"," E : 51.73410177230835점, S : 71.17260098457336점, G : 30.541467666625977점\n","5번 문장에 대한 esg 등급입니다.\n"," E : 56.460756063461304점, S : 96.29596471786499점, G : 4.019274190068245점\n","6번 문장에 대한 esg 등급입니다.\n"," E : 64.50883746147156점, S : 0.17146732425317168점, G : 99.81313347816467점\n","7번 문장에 대한 esg 등급입니다.\n"," E : 56.77765607833862점, S : 99.8140811920166점, G : 0.18708682619035244점\n","8번 문장에 대한 esg 등급입니다.\n"," E : 56.83636665344238점, S : 99.7882068157196점, G : 0.22728776093572378점\n","9번 문장에 대한 esg 등급입니다.\n"," E : 56.57462477684021점, S : 99.80804920196533점, G : 0.18187187379226089점\n","10번 문장에 대한 esg 등급입니다.\n"," E : 64.3987774848938점, S : 0.17293673008680344점, G : 99.80529546737671점\n"]}]},{"cell_type":"markdown","source":["`모델 저장하기`"],"metadata":{"id":"kUvVojwv5EoD"}},{"cell_type":"code","source":["# 저장하기(모델 추가 계층 및 옵티마이저)\n","torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, '/content/drive/MyDrive/model_checkpoint/test_version1.pt')"],"metadata":{"id":"llbTmXCcHNth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 재적용하기\n","model = BERT_ESG_Classifier(kobertmodel,  dr_rate = 0.5).to(device) # 모델 및 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n","\n","checkpoint = torch.load('/content/drive/MyDrive/model_checkpoint/test_version1.pt') # 전체 환경 load 하기\n","\n","model.load_state_dict(checkpoint['model_state_dict']) # 적용하기\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"],"metadata":{"id":"R4OQDVf_I5nx","executionInfo":{"status":"ok","timestamp":1713189453225,"user_tz":-540,"elapsed":1944,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"d914e8be-8048-475a-eff1-30081d04a1f9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# 재확인하기 ---> 결과 동일\n","test_data = pd.read_csv('/content/drive/MyDrive/kobert_modeling/naver_news_test.csv')\n","# 문장만 추출\n","sents = test_data['content']\n","for i, sent in enumerate(sents[:10]) :\n","    esg_output = predict(sent)\n","    print(f'{i+1}번 문장에 대한 esg 등급입니다.')\n","    print(f' E : {esg_output[0] * 100}점, S : {esg_output[1] * 100}점, G : {esg_output[2] * 100}점')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xQPKjtxImPR","outputId":"203aec6f-43af-446c-cb96-f24f2681222b","executionInfo":{"status":"ok","timestamp":1713191557421,"user_tz":-540,"elapsed":2714,"user":{"displayName":"한상혁","userId":"05319193929852015189"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["1번 문장에 대한 esg 등급입니다.\n"," E : 51.00491642951965점, S : 59.80818271636963점, G : 42.548397183418274점\n","2번 문장에 대한 esg 등급입니다.\n"," E : 64.41511511802673점, S : 0.1727480790577829점, G : 99.80581402778625점\n","3번 문장에 대한 esg 등급입니다.\n"," E : 64.37705159187317점, S : 0.174202723428607점, G : 99.82020854949951점\n","4번 문장에 대한 esg 등급입니다.\n"," E : 51.73410177230835점, S : 71.17260098457336점, G : 30.541467666625977점\n","5번 문장에 대한 esg 등급입니다.\n"," E : 56.460756063461304점, S : 96.29596471786499점, G : 4.019274190068245점\n","6번 문장에 대한 esg 등급입니다.\n"," E : 64.50883746147156점, S : 0.17146732425317168점, G : 99.81313347816467점\n","7번 문장에 대한 esg 등급입니다.\n"," E : 56.77765607833862점, S : 99.8140811920166점, G : 0.18708682619035244점\n","8번 문장에 대한 esg 등급입니다.\n"," E : 56.83636665344238점, S : 99.7882068157196점, G : 0.22728776093572378점\n","9번 문장에 대한 esg 등급입니다.\n"," E : 56.57462477684021점, S : 99.80804920196533점, G : 0.18187187379226089점\n","10번 문장에 대한 esg 등급입니다.\n"," E : 64.3987774848938점, S : 0.17293673008680344점, G : 99.80529546737671점\n"]}]},{"cell_type":"markdown","source":["`모델2 : model + esg + 긍부정`"],"metadata":{"id":"e_-nd387J6Z-"}},{"cell_type":"code","source":[],"metadata":{"id":"SHDnSP8QLMe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_a6pmKvBf1qY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QuANrPX3f1no"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nyvNPiZNf1lI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XymWrnCpf1ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RStsdt1Yf1f_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ecUBvyZrf1dY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pYdCYnAqf1SA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D1bMB9bWf1PZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qYggaVIsBJqx"},"source":["# 사용한 코드 설명 부분(실행하지 않는 부분)"]},{"cell_type":"markdown","metadata":{"id":"uVJppI5QBJqx"},"source":["`모델 저장하고 불러오기`\n","- 저장할 파라미터 대상 : 분류를 위해서 추가한 계층의 parameters [not kobert parameters]\n","- 저장할 때 확장자 : .pt or .pth\n","- 모델 저장 시 권장 방법 : torch.save(model.state_dict(), 경로)  -->  torch.save(model, 경로) 보다 가볍게 저장할 수 있음."]},{"cell_type":"markdown","source":["1. 모델의 일부 정보(state_dict())만 저장하고 불러오기 : 모델 저장 시 권장"],"metadata":{"id":"ifG4pkFfafzj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"src08b0xBJqx"},"outputs":[],"source":["# model의 state_dict() 저장하기\n","torch.save(model.classifier.state_dict(), '경로 입력')\n","\n","# model의 state_dict() 적용하기\n","model = BERT_ESG_Classifier(kobertmodel,  dr_rate = 0.5).to(device) # 1. 모델 선언\n","model.classifier.load_state_dict(torch.load('사전에 정의해둔 추가 계층의 파라미터의 파일을 저장한 경로')) # 2. 파라미터 적용\n","model.eval() # 3. 파이토치 공식문서 코멘트 : 꼭 model.eval()을 호출하여 드롭아웃 및 배치 정규화를 평가 모드로 설정하여야 합니다."]},{"cell_type":"markdown","source":["2. 전체 모델 저장하고 불러오기"],"metadata":{"id":"VtBZzXCTaqqd"}},{"cell_type":"code","source":["# 전체 모델 저장하기\n","torch.save(model, '경로 입력')\n","\n","# 전체 모델 적용하기\n","model = BERT_ESG_Classifier(kobertmodel,  dr_rate = 0.5).to(device) # 1. 모델 선언\n","model = torch.load('저장한 경로')\n","model.eval()"],"metadata":{"id":"Owd34bVtaqUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. 모델 + 학습 환경 저장하고 불러오기"],"metadata":{"id":"HZju_sFpbdHi"}},{"cell_type":"code","source":["# 모델 + 학습 환경 저장하기\n","torch.save({'epoch': num_epochs, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss}, '경로 입력')\n","\n","# 모델 + 학습 환경 적용하기\n","model = BERT_ESG_Classifier(kobertmodel,  dr_rate = 0.5).to(device) # 모델 및 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n","\n","checkpoint = torch.load('저장한 경로 입력') # 전체 환경 load 하기\n","\n","model.load_state_dict(checkpoint['model_state_dict']) # 적용하기\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","\n","model.eval()\n","# - or -\n","model.train()"],"metadata":{"id":"WmXBxLJqbktn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 추가로 적용할 수 있는 참고 사항\n","4. 여러개 모델 하나의 파일에 저장하고 불러오기\n","5. 다른 모델의 매개변수를 사용하여 빠르게 모델 시작하기(warmstart)\n","6. GPU에서 저장하고 CPU에서 불러오기\n","7. GPU에서 저장하고 GPU에서 불러오기\n","8. CPU에서 저장하고 GPU에서 불러오기"],"metadata":{"id":"4AhdBdXxchEL"}},{"cell_type":"markdown","source":["`구글 코랩에서 모델 저장하고 불러오기`"],"metadata":{"id":"PcK0IeAp8Jct"}},{"cell_type":"code","source":["PATH = '/content/drive/MyDrive/Colab Notebooks/inceptionv4.pt' # 경로 입력(예시)\n","import os.path\n","\n","epoch_start = 1\n","\n","if os.path.exists(PATH): # 해당 경로에 파일이 있으면\n","    checkpoint = torch.load(PATH)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch_start = checkpoint['epoch'] + 1\n","    print(\"successfully loaded!\")\n","    print(\"epoch saved until here: \", epoch_start-1)\n","    print(\"train starts from this epoch: Epoch \", epoch_start)"],"metadata":{"id":"D2eRTRVz8MXe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCdKKwqEBJqy"},"source":["`pytorch에서 모델의 각 계층 이름 or 파라미터(가중치, 편향) 수치에 직접적으로 접근하는 방법`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTbbi6EBBJq2"},"outputs":[],"source":["# torch.nn.Module.parameters() : 모델이 가지고 있는 가중치와 편향을 순서대로 보여준다.\n","for para in model.parameters() :\n","    print(para)\n","\n","# torch.nn.Module.named_parameters() : 모델이 가지고 있는 (layer의 '이름', 해당 layer의 'parameter')를 순서대로 보여준다.\n","for name, para in model.named_parameters() :\n","    print(name)\n","    print(para)"]},{"cell_type":"markdown","source":["`pytorch에서 모델의 각 계층 이름 or 파라미터(가중치, 편향) 요약 정보에 접근하는 방법`"],"metadata":{"id":"RRL2dFUuPNJ8"}},{"cell_type":"code","source":["# model.children() : 계층의 특성을 요약해서 보여준다.\n","for child in model.children() :\n","    print(child)\n","\n","# model.named_children() : 각 계층의 이름과 요약 정보를 보여준다\n","for name, child in model.named_children() :\n","    print(name, child)"],"metadata":{"id":"M2g_sQd9NWpH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`모델 내 존재하는 특정 계층의 파라미터를 보는 방법 - 1 : get_parameter()`"],"metadata":{"id":"FVhUQOHrPdWj"}},{"cell_type":"code","source":["# model이 가지고 있는 layer 이름 파악하기\n","for name, para in model.named_parameters() :\n","    print(name)\n","\n","# 특정 layer의 파라미터 찾기\n","model.get_parameter('모델 내에 존재하는 특정 계층의 이름')"],"metadata":{"id":"2tavw0YRNtpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`모델 내 존재하는 특정 계층의 파라미터를 보는 방법 - 2 : state_dict() : {계층 이름 : 해당 계층 파라미터 값}`\n","- model.state_dict()\n","- optimizer.state_dict()"],"metadata":{"id":"yvq1UFLBQ1S6"}},{"cell_type":"code","source":["# {계층이름 : 해당 계층 파라미터 값}을 가지는 객체 생성\n","state_dict = model.state_dict()\n","# 특정 계층의 파라미터 접근하는 법\n","state_dict['model에 존재하는 계층 이름']\n"],"metadata":{"id":"0eHS8yyyQVVo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`model.children() vs model.modules()`\n","- children : 시퀀스 단계 별 특성 정보\n","- module : 시퀀스 이름 + 시퀀스 단계 별 특성 정보 + 시퀀스 요약 정보"],"metadata":{"id":"GiLXZul3UeTd"}},{"cell_type":"code","source":["# 이터레이터이고 직접적으로 보고자 할 때는 list로 바꿔야 한다\n","print(list(model.children()))\n","print(list(model.modules()))"],"metadata":{"id":"pTV6NH6MQx9N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`특정 계층만 freezing 하는 방법`\n","- freezing의 목적 : pre-trained model을 가지고 와서 추가 학습을 시킬 때 pre-trained 모델의 parameter가 업데이트 되는 경우 기존의 잘 학습된 특성을 잃어버릴 수 있기 때문\n","- freezing 방식 : pre-trained 된 부분의 파라미터는 freezing 하고 특정 task를 위해 추가로 쌓은 layer만 파라미터 업데이트를 할 수 있도록(학습할 수 있도록) 설정한다.\n","- freezing 판단 : 파라미터의 requires_grad(기울기 계산 할거야?)가 True : unfreezing, False : freezing  "],"metadata":{"id":"7zH2endAhH5P"}},{"cell_type":"code","source":["# requires_grad 접근 방법 : 1. model의 파라미터에 접근 2. model파라미터.requires_grad\n","for name, param in model.named_parameters():\n","    if name.count(\"fc2\"): # 내가 freezing 하기를 원하는 계층이 있으면 1 이상의 숫자가 나오면서 requires_grad = False로 설정함.\n","        param.requires_grad = False"],"metadata":{"id":"5eM_I-O4UL6S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`model.train() vs model.eval() & with torch.no_grad()`"],"metadata":{"id":"-DIU8hE2ob31"}},{"cell_type":"code","source":["# 학습 모드\n","model.train()\n","\n","# 평가 모드\n","model.eval()\n","with torch.no_grad() :\n","    pass"],"metadata":{"id":"n2eraLzdokP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`model.eval() & with torch.no_grad()이 항상 같이 등장하는 이유`\n","- model.eval() : 평가(테스트) 모드\n","- with torch.no_grad() : 아래부터는 기울기 계산을 하지 않는다.\n","- 항상 같이 등장하는 이유 : 평가 모드에서는 기울기를 계산할 필요가 없다. 하지만 model.eval() 모드를 작동해도 requires_grad = False 설정이 되지는 않는다. 단지  dropout, batchnorm 같은 것의 기능을 끄는 역할만 한다. 따라서 with torch.no_grad를 이용한다. 이는 requires_grad = False가 되도록 하여 불필요한 계산을 줄이는 역할을 한다."],"metadata":{"id":"DVKNrkrlmuPj"}},{"cell_type":"markdown","source":["`model.zero_grad()와 optimizer.zero_grad() 차이`\n","- 모델 학습 시 '한 종류'의 optimizer만 사용되는 경우 : optimizer.zero_grad() 사용 권장\n","- 모델 학습 시 '여러 종류'의 optimizer만 사용되는 경우 : model.zero_grad() 사용 권장\n","- zero_grad() 사용 이유\n","    - 1. loss.backward()로 Loss gradient를 역전파 한다.\n","    - 2. 기울기는 누적되어 저장이 된다.\n","    - 3. zero_grad()를 이용해서 기울기 값을 초기화시켜 이전 기울기의 영향을 제거한다."],"metadata":{"id":"1SaIhfjGuWCK"}},{"cell_type":"markdown","source":["`학습 시 일반적으로 필요한 것들`\n","- 1. Dataset & DataLoader\n","- 2. model\n","- 3. loss_function\n","- 3. optimizer\n","- 4. scheduler"],"metadata":{"id":"F1FcMBdRvw-M"}},{"cell_type":"markdown","source":["`학습(train) 순서`\n","- 1. model 정의하기\n","- 2. loss_function 정의하기\n","- 3. optimizer와 scheduler 정의하기\n","- 4. optimizer.zero_grad() : epoch 마다 실행\n","- 5. model 순전파\n","- 6. loss 구하기\n","- 7. loss.backward() : 손실함수 기울기 계산\n","- 8. optimizer.step() / scheduler.step() : 기울기 기반 가중치 업데이트 / 학습률 조정 업데이트\n"],"metadata":{"id":"iear5b74wN1E"}},{"cell_type":"markdown","source":["`train_test_split 사용방법`\n","\n","- 1. split 대상 : 리스트, 튜플, 데이터프레임\n","    - split의 대상이 하나(x)인 경우 : x_train, x_test 반환\n","    - split의 대상이 두 개(x,y)인 경우 : x_train, x_test, y_train, y_test 반환\n","\n","- 2. 주요 파라미터\n","    -  test_size : ex) test_size = 0.2 --> 훈련 데이터 80%, 평가 데이터 20%\n","    - shuffle : shuffle = True(순서를 무작위로 섞기), shuffle = False(순서대로)\n","    - random_state = int숫자 : 매번 '동일한' '훈련 데이터'와 '평가 데이터'를 얻기 위한 설정\n","    - stratify : DataFrame을 split 하는 경우 --> 특정 칼럼을 지정하고 지정 칼럼의 값의 비율을 동일하게 만드는 역할(블로그 참고)"],"metadata":{"id":"vBalRIb8Htd_"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6521480233c5471887a798dc9dc6cc37":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4887f5e47e8a4a2587d8af6dca8ac290","IPY_MODEL_a4445e0346fe41019a3b0e17cfd9fe4b","IPY_MODEL_a5d718a83abd4205b3489d4a4d2e5395"],"layout":"IPY_MODEL_eafc6a3ca3a1438381d1a504a5f25603"}},"4887f5e47e8a4a2587d8af6dca8ac290":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c212f2dc79584bb1ab21a0fd526aefb9","placeholder":"​","style":"IPY_MODEL_4cb5f789c1ca40df8e2b576682ed9465","value":"100%"}},"a4445e0346fe41019a3b0e17cfd9fe4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10f7e3f2aed74451a3431fc862d9ef6f","max":951,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3939000377a144cab85f69040daed1d1","value":951}},"a5d718a83abd4205b3489d4a4d2e5395":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bea45ac188634d8cb4c8b33820978dc8","placeholder":"​","style":"IPY_MODEL_924373d1abf3449fa41bb0191f0c8318","value":" 951/951 [18:17&lt;00:00,  1.13s/it]"}},"eafc6a3ca3a1438381d1a504a5f25603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c212f2dc79584bb1ab21a0fd526aefb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cb5f789c1ca40df8e2b576682ed9465":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10f7e3f2aed74451a3431fc862d9ef6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3939000377a144cab85f69040daed1d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bea45ac188634d8cb4c8b33820978dc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"924373d1abf3449fa41bb0191f0c8318":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}