{"cells":[{"cell_type":"markdown","metadata":{"id":"ColxfM_LfK4K"},"source":["# KOBERT를 이용해 감정 분류하기\n"]},{"cell_type":"markdown","metadata":{"id":"xmjhbDCEumPC"},"source":["`필요 환경 및 패키지 설치`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSkf0n9pfK4N"},"outputs":[],"source":["!pip install mxnet\n","!pip install pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch\n","!pip install gluonnlp==0.10.0\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1KvuoVJkrgM"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","import urllib.request\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","kobert_tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","\n","word_tokenizer = kobert_tokenizer.tokenize\n","\n","kobertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(kobert_tokenizer.vocab_file, padding_token='[PAD]')"]},{"cell_type":"markdown","source":["`사전 파라미터 지정`"],"metadata":{"id":"VuMpkitVIgXl"}},{"cell_type":"code","source":["max_len = 128\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"metadata":{"id":"2ttg1EPIIfem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`긍부정 데이터셋 불러오기`"],"metadata":{"id":"rZgT_mNlEC5u"}},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n","\n","# 긍정 1 부정 0\n","train_data = pd.read_table('ratings_train.txt')\n","test_data = pd.read_table('ratings_test.txt')\n","\n","\n","# pn_train_data\n","pn_train_data_list = []\n","for sent, label in zip(train_data['document'], train_data['label']):\n","    pn_train_data = []\n","    pn_train_data.append(str(sent))\n","    if label == 1 :\n","        pn_train_data.append((1,0))\n","    elif label == 0 :\n","        pn_train_data.append((0,1))\n","\n","    pn_train_data_list.append(pn_train_data)\n","\n","# pn_test_data\n","pn_test_data_list = []\n","for sent, label in zip(test_data['document'], test_data['label']):\n","    pn_test_data = []\n","    pn_test_data.append(str(sent))\n","    if label == 1 :\n","        pn_test_data.append((1,0))\n","    elif label == 0 :\n","        pn_test_data.append((0,1))\n","\n","    pn_test_data_list.append(pn_test_data)"],"metadata":{"id":"zJXfEAuIEJaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pn_test_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OCzwnIZQTaa","executionInfo":{"status":"ok","timestamp":1713254281566,"user_tz":-540,"elapsed":420,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"5ff837db-211a-4efb-87ed-64cf824023fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['마무리는 또 왜이래', (0, 1)]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["`pn_dataset 구축 및 dataloader 구축`"],"metadata":{"id":"IEHgwas0_dMv"}},{"cell_type":"code","source":["class SENT_BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n","                 pad, pair):\n","        self.transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab = vocab, pad=pad, pair=pair)\n","        self.sentences = [self.transform([i[sent_idx]]) for i in dataset]\n","        self.labels = torch.tensor([(int(i[label_idx][0]), int(i[label_idx][1])) for i in dataset])\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","\n","pn_data_train = SENT_BERTDataset(pn_train_data_list, 0, 1, word_tokenizer, vocab, max_len, True, False)\n","pn_data_test = SENT_BERTDataset(pn_test_data_list, 0, 1, word_tokenizer, vocab, max_len, True, False)\n","\n","# DataLoader의 역할 : 한번에 batch_size만큼 시키는 iterable 생성\n","pn_train_dataloader = torch.utils.data.DataLoader(pn_data_train, batch_size = batch_size, num_workers = 5)\n","pn_test_dataloader = torch.utils.data.DataLoader(pn_data_test, batch_size = batch_size, num_workers = 5)"],"metadata":{"id":"2L-IawEw_ZVA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ESG 모델 + 긍부정 분류 모델"],"metadata":{"id":"0mpZUIwCeDqQ"}},{"cell_type":"code","source":["class BERT_SENT_Classifier(nn.Module):\n","    def __init__(self,\n","                 bert, # bert 모델 받아오기\n","                 hidden_size = 768, # 은닉층의 크기(기준 수)\n","                 num_classes = 2,   # [E,S,G]\n","                 dr_rate = None, # dropout_rate : 신경망 학습 중에 일부 뉴런을 무작위로 제거하여 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법\n","                 params = None):\n","        super(BERT_SENT_Classifier, self).__init__()\n","        self.bert = bert # 사용할 bert 모델 지정\n","        self.dr_rate = dr_rate # dropout 비율 지정\n","        self.sent_classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate: # dr_rate가 0과 None이 아니고 (0~1) 사이로 설정되었을 경우에는\n","            self.dropout = nn.Dropout(p = dr_rate) # nn.dropout을 실행한다.\n","        self.activation_softmax = nn.Softmax(dim = 1)\n","\n","\n","\n","    def gen_attention_mask(self, token_ids, valid_length): # attention mask를 생성하는 것 : 이 문장을 바라보는 전문가들 생성한다.\n","        attention_mask = torch.zeros_like(token_ids) # token_ids와 같은 크기를 가지고 있는 0으로 채워지는 것들  : 뭔가 포지셔널 인코딩일 것 같은 느낌\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1 # 가리지 않는 부분 설정하기\n","        return attention_mask.float() # dtype 은 float32로 한다.\n","\n","    def forward(self, token_ids, valid_length, segment_ids): # segment_ids 는 문장단위를 나누는 임베딩 부분인 것 같다. 한 문장일 경우 0000, 두 문장 이상일 경우 00..111\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        # torch.long() : int64 타입\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict = False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        posneg = self.sent_classifier(out)\n","        activation_posneg = self.activation_softmax(posneg)\n","\n","        return activation_posneg"],"metadata":{"id":"A6Xe0yNUeC1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTa0AGa3BJqv"},"source":["`모델 정의하기`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HB9xXUOow9ej"},"outputs":[],"source":["# BERT  모델 불러오기\n","model = BERT_SENT_Classifier(kobertmodel,  dr_rate = 0.5).to(device)\n","\n","\n","\n","# kober model + esg_classifier freezing 하기\n","for name, para in model.named_parameters() :\n","    if not name.count('sent_classifier') :\n","        para.requires_grad = False\n","\n","# 옵티마이저 생성 시 전달해줄 파라미터 정의\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [weight for name, weight in model.named_parameters() if not any(nd in name for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [weight for name, weight in model.named_parameters() if any(nd in name for nd in no_decay)], 'weight_decay': 0.0} ]\n","\n","\n","# 옵티마이저 정의\n","optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n","\n","# 손실함수 정의\n","loss_fn = nn.BCELoss()\n","\n","# 스케쥴러 생성 시 전달해줄 파라미터 정의\n","t_total = len(pn_train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","# 스케쥴러 정의\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)"]},{"cell_type":"markdown","metadata":{"id":"ksWVE9G-BJqw"},"source":["`긍부정 정확도 계산 함수 정의`"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"yMAGcjy95fHa","executionInfo":{"status":"ok","timestamp":1713257180567,"user_tz":-540,"elapsed":299,"user":{"displayName":"한상혁","userId":"05319193929852015189"}}},"outputs":[],"source":["# 긍부정 분류 시에 사용한다.\n","def sent_calc_accuracy(X,Y):\n","    count =  0\n","    for pred, result in zip(out, label) :\n","\n","        temp = []\n","        for p in pred :\n","            if p < 0.5 :\n","                temp.append(0)\n","            else :\n","                temp.append(1)\n","        temp = torch.tensor(temp)\n","        if abs(temp.sub(result)).sum():\n","            count +=1\n","    train_acc = count/batch_size\n","    return train_acc\n",""]},{"cell_type":"markdown","metadata":{"id":"d-SfDpRK12-v"},"source":["`모델 학습 코드`"]},{"cell_type":"code","source":["# 감정 분류 시에 사용한다.\n","for e in range(1):\n","    train_acc = 0.0\n","    model.train() # model을 훈련모드로 바꾸고, 가중치가 업데이트 될 수 있게 한다.\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(pn_train_dataloader)):\n","        # 옵티마이저의 미분값을 0으로 초기화\n","        optimizer.zero_grad()\n","\n","        # model의 forward 인자 설정\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.float().to(device)\n","\n","        # model output 도출\n","        out = model.forward(token_ids, valid_length, segment_ids)\n","        break\n","        # 모델 output과 label(정답)과의 손실함수 정의\n","        loss = loss_fn(out, label)\n","\n","        # 손실함수의 기울기 계산\n","        loss.backward()\n","\n","        # gradient vanishing 또는 gradient exploding을 방지하기 위한 gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","        # 기울기 반영한 가중치 업데이트\n","        optimizer.step()\n","        scheduler.step()\n","\n","        train_acc += sent_calc_accuracy(out, label)\n","\n","        if batch_id % log_interval == 0:\n","             print(f'epoch : {e+1} | batch_id : {batch_id + 1} | loss : {loss.data.cpu().numpy()}| accuracy : {train_acc / batch_size}')\n","\n","    print(\"epoch : {} train acc : {}\".format(e+1, train_acc / batch_size))"],"metadata":{"id":"ihR-KZotr3cN","colab":{"base_uri":"https://localhost:8080/","height":216,"referenced_widgets":["fbd66288a5fe49da9fa19562e0185667","67c63eeeab354a39926ed1fe7043143d","32ebba338e2542c3acee03c20e13ab43","95f676d183df41e994c1696f3bfd7b25","b4ef70877f2540b285ffedbb5586c7c4","320c17131e834eaaa3e3699c37854cf5","4061551dfc1d4abe9ab11cc63b964f18","6435743eac014fdc9f516802b5477ff2","9d8a5883928d40f0964eadf38dae6610","47454bd08af44c5a8b868f4945a4448b","1b56a39e0d3b445ca6689312bd04023b"]},"executionInfo":{"status":"ok","timestamp":1713257263854,"user_tz":-540,"elapsed":33081,"user":{"displayName":"한상혁","userId":"05319193929852015189"}},"outputId":"b319b27b-dba6-492e-c42b-701f2cdb94cf"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-26-cc05c8ee5d42>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(pn_train_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2344 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd66288a5fe49da9fa19562e0185667"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["epoch : 1 train acc : 0.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"SXsq_T5z2JvX"},"source":["`학습 완료시킨 model을 이용한 predict 함수`"]},{"cell_type":"code","source":["def sent_predict(predict_sentence): # input = 감정분류하고자 하는 sentence\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","\n","    another_test = SENT_BERTDataset(dataset_another, 0, 1, word_tokenizer, vocab, max_len, True, False) # 토큰화한 문장\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size = batch_size, num_workers = 5) # torch 형식 변환\n","\n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        # model의 forward 인자 설정\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","        # model output 도출\n","        out = model.forward(token_ids, valid_length, segment_ids)\n","\n","\n","        test_eval = []\n","        for i in out: # out = model(token_ids, valid_length, segment_ids)\n","            logits = i\n","            logits = logits.detach().cpu().numpy()\n","\n","            if np.argmax(logits) == 0 :\n","                test_eval.append(f'긍정 문장 : 문장의 긍정 수치 : {logits[0]}, 문장의 부정 수치 : {logits[1]}')\n","            elif np.argmax(logits) == 1 :\n","                test_eval.append(f'긍정 문장 : 문장의 긍정 수치 : {logits[0]}, 문장의 부정 수치 : {logits[1]}')\n","\n","\n","\n","    return test_eval[0]"],"metadata":{"id":"yc-IGeh-sPT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`sent_predict 해보기"],"metadata":{"id":"Q-nAgAupVjTg"}},{"cell_type":"code","source":["# 테스트 데이터 불러오기\n","test_data = pd.read_csv('/content/drive/MyDrive/kobert_modeling/naver_news_test.csv')\n","\n","# 문장 추출\n","sents = test_data['content']\n","for i, sent in enumerate(sents[:10]) :\n","    esg_output = sent_predict(sent)\n","    print(f'{i+1}번 문장은' + esg_output)"],"metadata":{"id":"yNso-PlWViwI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`모델 저장하기`"],"metadata":{"id":"kUvVojwv5EoD"}},{"cell_type":"code","source":["# 저장하기(모델 추가 계층 및 옵티마이저)\n","torch.save({'model_sent_classifier.state_dict': model.sent_classifier.state_dict()}, '/content/drive/MyDrive/model_checkpoint/sent_version1.pt')"],"metadata":{"id":"llbTmXCcHNth"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fbd66288a5fe49da9fa19562e0185667":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67c63eeeab354a39926ed1fe7043143d","IPY_MODEL_32ebba338e2542c3acee03c20e13ab43","IPY_MODEL_95f676d183df41e994c1696f3bfd7b25"],"layout":"IPY_MODEL_b4ef70877f2540b285ffedbb5586c7c4"}},"67c63eeeab354a39926ed1fe7043143d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_320c17131e834eaaa3e3699c37854cf5","placeholder":"​","style":"IPY_MODEL_4061551dfc1d4abe9ab11cc63b964f18","value":"  0%"}},"32ebba338e2542c3acee03c20e13ab43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6435743eac014fdc9f516802b5477ff2","max":2344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d8a5883928d40f0964eadf38dae6610","value":0}},"95f676d183df41e994c1696f3bfd7b25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47454bd08af44c5a8b868f4945a4448b","placeholder":"​","style":"IPY_MODEL_1b56a39e0d3b445ca6689312bd04023b","value":" 0/2344 [00:32&lt;?, ?it/s]"}},"b4ef70877f2540b285ffedbb5586c7c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"320c17131e834eaaa3e3699c37854cf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4061551dfc1d4abe9ab11cc63b964f18":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6435743eac014fdc9f516802b5477ff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d8a5883928d40f0964eadf38dae6610":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47454bd08af44c5a8b868f4945a4448b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b56a39e0d3b445ca6689312bd04023b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}