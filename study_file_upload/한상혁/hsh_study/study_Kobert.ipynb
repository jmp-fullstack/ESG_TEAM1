{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kobert / Pytorch 공부용 ipynb 파일\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kobert를 사용하기 위한 kobert_transformers 패키지 다운로드\n",
    "\n",
    "출처 : https://github.com/monologg/KoBERT-Transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/monologg/KoBERT-Transformers/master/kobert_transformers/tokenization_kobert.py -O ./tokenization_kobert.py\n",
    "!pip3 install kobert-transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 불러오기 : 사용 목적 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenization_kobert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader \u001b[38;5;66;03m# 데이터를 모델에 공급하고 학습을 진행\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkobert_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_kobert_model, get_distilkobert_model \u001b[38;5;66;03m# 텍스트 분류나 자연어 이해 작업\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenization_kobert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KoBertTokenizer \u001b[38;5;66;03m# KoBERT를 위한 토크나이저\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#transformers\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW \u001b[38;5;66;03m# 모델의 가중치를 최적화 : LOSS 최소화\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tokenization_kobert'"
     ]
    }
   ],
   "source": [
    "import torch # 텐서 연산과 자동 미분을 지원하여 모델을 구축하고 학습\n",
    "from torch import nn # 신경망 레이어를 정의하는 데 사용\n",
    "from torch.utils.data import Dataset, DataLoader # 데이터를 모델에 공급하고 학습을 진행\n",
    "\n",
    "from kobert_transformers import get_kobert_model, get_distilkobert_model # 텍스트 분류나 자연어 이해 작업\n",
    "from tokenization_kobert import KoBertTokenizer # KoBERT를 위한 토크나이저\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW # 모델의 가중치를 최적화 : LOSS 최소화\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup # 학습률을 조정(schedule)해서 안정적인 학습 유도\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화 리뷰 train/test 파일 가지고 데이터셋 구축\n",
    "- ids : 아이디 따로 모으기\n",
    "- review_composition : 리뷰 텍스트 따로 모으기\n",
    "- labels : 라벨 따로 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ratings_test.csv', 'r', encoding = 'utf-8') as test_file :\n",
    "    test_reviews = test_file.readlines()\n",
    "\n",
    "with open('./ratings_train.csv', 'r', encoding = 'utf-8') as train_file :\n",
    "    train_reviews = train_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [test_review.split()[0] for test_review in test_reviews]\n",
    "test_review_composition = [' '.join(test_review.split()[1:-1]) for test_review in test_reviews]\n",
    "test_labels = [ test_review.split()[-1] for test_review in test_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [train_review.split()[0] for train_review in train_reviews]\n",
    "train_review_composition = [' '.join(train_review.split()[1:-1]) for train_review in train_reviews]\n",
    "train_labels = [ train_review.split()[-1] for train_review in train_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
